{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion-based Super-Resolution for EuroSAT\n",
        "\n",
        "Training diffusion models for 13-channel multispectral satellite imagery super-resolution.\n",
        "\n",
        "**Current Best Result:** 19.31 dB PSNR, 0.7379 SSIM (epoch 55)"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q torchgeo lpips"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('diffusion-sr-eurosat'):\n",
        "    !git clone https://github.com/ParkerJ1/diffusion-sr-eurosat.git\n",
        "\n",
        "%cd diffusion-sr-eurosat\n",
        "print(\"In project directory:\", os.getcwd())\n",
        "\n",
        "!pip install -q torchgeo lpips\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from configs.config import BaseConfig\n",
        "from src.data.dataset import get_dataloader\n",
        "from src.models.unet import ConditionalUNet\n",
        "from src.models.diffusion import Scheduler\n",
        "from src.training.train import train\n",
        "from src.training.sample import sampling\n",
        "from src.evaluation.metrics import metrics\n",
        "from src.utils.visualisation import visualize_and_save_samples\n",
        "from src.utils.helpers import find_latest_model"
      ],
      "metadata": {
        "id": "clone_repo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ccfe5e4-974f-47c7-caae-34807bab5965"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusion-sr-eurosat'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 41 (delta 5), reused 32 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (41/41), 35.39 KiB | 11.80 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "/content/diffusion-sr-eurosat/diffusion-sr-eurosat/diffusion-sr-eurosat\n",
            "In project directory: /content/diffusion-sr-eurosat/diffusion-sr-eurosat/diffusion-sr-eurosat\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "imports_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Import from our modules\n",
        "from configs.config import BaseConfig\n",
        "from src.data.dataset import get_dataloader\n",
        "from src.models.unet import ConditionalUNet\n",
        "from src.models.diffusion import Scheduler\n",
        "from src.training.train import train\n",
        "from src.training.sample import sampling\n",
        "from src.evaluation.metrics import metrics\n",
        "from src.utils.visualisation import visualize_and_save_samples\n",
        "from src.utils.helpers import find_latest_model\n",
        "\n",
        "print(\"All imports successful!\")"
      ],
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84864f51-c195-4942-f27f-fdaf9473ed6b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create configuration\n",
        "config = BaseConfig()\n",
        "\n",
        "# Override any settings for this experiment\n",
        "# config.EPOCHS = 200\n",
        "# config.BATCH_SIZE = 256\n",
        "# config.LR = 1e-4\n",
        "\n",
        "print(f\"Device: {config.DEVICE}\")\n",
        "print(f\"Dataset: {config.DATASET}\")\n",
        "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
        "print(f\"Epochs: {config.EPOCHS}\")\n",
        "print(f\"Learning Rate: {config.LR}\")\n",
        "print(f\"Output Directory: {config.OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f43177c-9094-4023-f78f-f81b08b69ef3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Dataset: EuroSAT\n",
            "Batch Size: 128\n",
            "Epochs: 100\n",
            "Learning Rate: 5e-05\n",
            "Output Directory: /content/drive/MyDrive/SharedColab/DiffusionSR_EuroSAT/output_20251219_123656/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Model & Data"
      ],
      "metadata": {
        "id": "init_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directories\n",
        "if config.TRAIN:\n",
        "    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(config.DATASET_PATH, exist_ok=True)\n",
        "    print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
        "\n",
        "# Get dataloader\n",
        "dataloader = get_dataloader(config)\n",
        "print(f\"âœ… Dataloader created: {len(dataloader)} batches\")\n",
        "\n",
        "# Get validation batch\n",
        "val_high_res_img, val_low_res_img = next(iter(dataloader))\n",
        "val_high_res_img = val_high_res_img[:config.NUM_EVAL_SAMPLES].to(config.DEVICE)\n",
        "val_low_res_img = val_low_res_img[:config.NUM_EVAL_SAMPLES].to(config.DEVICE)\n",
        "print(f\"âœ… Validation batch: {val_high_res_img.shape}\")"
      ],
      "metadata": {
        "id": "init_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21f0711-75af-4657-fb5e-864ed47e8c82"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory: /content/drive/MyDrive/SharedColab/DiffusionSR_EuroSAT/output_20251219_123656/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 314k/314k [00:00<00:00, 12.9MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.07G/2.07G [00:32<00:00, 63.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded channel statistics for normalization.\n",
            "âœ… Dataloader created: 127 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Validation batch: torch.Size([16, 13, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = ConditionalUNet(config).to(config.DEVICE)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config.LR,\n",
        "    weight_decay=config.WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Initialize scheduler\n",
        "scheduler = Scheduler()\n",
        "alphas_cumprod = scheduler.get_linear_beta_scheduler(\n",
        "    config.TIMESTEPS,\n",
        "    config.DEVICE\n",
        ")\n",
        "\n",
        "# Initialize metrics\n",
        "psnr_metric = metrics.PSNR(data_range=2.0)  # [-1, 1] range\n",
        "ssim_metric = metrics.SSIM(data_range=2.0)\n",
        "lpips_metric = metrics.LPIPS()\n",
        "\n",
        "print(f\"âœ… Model initialized: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"âœ… Optimizer: AdamW (lr={config.LR})\")\n",
        "print(f\"âœ… Scheduler: Linear beta ({config.TIMESTEPS} timesteps)\")"
      ],
      "metadata": {
        "id": "init_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "673c5689-4b81-475e-b884-0f6490b49daf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ConditionalUNet.__init__() missing 2 required positional arguments: 'time_embed_dim' and 'config'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1363996434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConditionalUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer = optim.AdamW(\n",
            "\u001b[0;31mTypeError\u001b[0m: ConditionalUNet.__init__() missing 2 required positional arguments: 'time_embed_dim' and 'config'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load existing model if in inference mode\n",
        "if not config.TRAIN:\n",
        "    if config.MODEL_PATH is None:\n",
        "        # Auto-detect latest model\n",
        "        config.MODEL_PATH = find_latest_model(config.PROJECT_ROOT)\n",
        "\n",
        "    if config.MODEL_PATH and os.path.exists(config.MODEL_PATH):\n",
        "        checkpoint = torch.load(config.MODEL_PATH, map_location=config.DEVICE)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"âœ… Loaded model from: {config.MODEL_PATH}\")\n",
        "    else:\n",
        "        print(\"âš ï¸  No model found, starting from scratch\")\n",
        "        config.TRAIN = True"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if config.TRAIN:\n",
        "    print(\"Starting training...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{config.EPOCHS} ---\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        avg_loss = train(\n",
        "            model=model,\n",
        "            dataloader=dataloader,\n",
        "            scheduler=scheduler,\n",
        "            optimizer=optimizer,\n",
        "            device=config.DEVICE,\n",
        "            epoch=epoch\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch: {epoch+1} - Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Sample and evaluate\n",
        "        if (epoch + 1) % config.SAMPLE_EVERY == 0:\n",
        "            print(f\"---Sampling - epoch {epoch+1}\")\n",
        "\n",
        "            # Generate samples\n",
        "            generated_images = sampling(\n",
        "                model=model,\n",
        "                low_res_imgs=val_low_res_img,\n",
        "                scheduler=scheduler,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            # Compute metrics\n",
        "            psnr = psnr_metric(generated_images, val_high_res_img)\n",
        "            ssim = ssim_metric(generated_images, val_high_res_img)\n",
        "            lpips_score = lpips_metric(generated_images, val_high_res_img)\n",
        "\n",
        "            print(f\"PSNR: {psnr:.2f} dB | SSIM: {ssim:.4f} | LPIPS: {lpips_score:.4f}\")\n",
        "\n",
        "            # Save samples and visualizations\n",
        "            visualize_and_save_samples(\n",
        "                generated_images=generated_images,\n",
        "                val_high_res_img=val_high_res_img,\n",
        "                val_low_res_img=val_low_res_img,\n",
        "                epoch=epoch+1,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            # Save checkpoint\n",
        "            checkpoint_path = os.path.join(config.OUTPUT_DIR, f'model_epoch_{epoch+1}.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "                'psnr': psnr,\n",
        "                'ssim': ssim,\n",
        "                'lpips': lpips_score,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    # Save final model\n",
        "    final_model_path = os.path.join(config.OUTPUT_DIR, 'model.pth')\n",
        "    torch.save({\n",
        "        'epoch': config.EPOCHS,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, final_model_path)\n",
        "    print(f\"\\nâœ… Training complete! Final model saved: {final_model_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping training (config.TRAIN = False)\")"
      ],
      "metadata": {
        "id": "training_loop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation / Inference"
      ],
      "metadata": {
        "id": "eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate samples (works in both training and inference mode)\n",
        "print(\"Generating samples...\")\n",
        "\n",
        "generated_images = sampling(\n",
        "    model=model,\n",
        "    low_res_imgs=val_low_res_img,\n",
        "    scheduler=scheduler,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Compute metrics\n",
        "psnr = psnr_metric(generated_images, val_high_res_img)\n",
        "ssim = ssim_metric(generated_images, val_high_res_img)\n",
        "lpips_score = lpips_metric(generated_images, val_high_res_img)\n",
        "\n",
        "print(f\"\\nðŸ“Š Evaluation Results:\")\n",
        "print(f\"PSNR:  {psnr:.2f} dB\")\n",
        "print(f\"SSIM:  {ssim:.4f}\")\n",
        "print(f\"LPIPS: {lpips_score:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "visualize_and_save_samples(\n",
        "    generated_images=generated_images,\n",
        "    val_high_res_img=val_high_res_img,\n",
        "    val_low_res_img=val_low_res_img,\n",
        "    epoch='final',\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Analysis (Optional)"
      ],
      "metadata": {
        "id": "analysis_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on larger test set (optional)\n",
        "num_test_samples = 32\n",
        "\n",
        "print(f\"Evaluating on {num_test_samples} samples...\")\n",
        "\n",
        "all_psnr = []\n",
        "all_ssim = []\n",
        "all_lpips = []\n",
        "\n",
        "for i, (high_res, low_res) in enumerate(dataloader):\n",
        "    if i >= num_test_samples // config.BATCH_SIZE:\n",
        "        break\n",
        "\n",
        "    high_res = high_res.to(config.DEVICE)\n",
        "    low_res = low_res.to(config.DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated = sampling(model, low_res, scheduler, config)\n",
        "\n",
        "    all_psnr.append(psnr_metric(generated, high_res))\n",
        "    all_ssim.append(ssim_metric(generated, high_res))\n",
        "    all_lpips.append(lpips_metric(generated, high_res))\n",
        "\n",
        "import numpy as np\n",
        "print(f\"\\nðŸ“Š Results on {num_test_samples} samples:\")\n",
        "print(f\"PSNR:  {np.mean(all_psnr):.2f} Â± {np.std(all_psnr):.2f} dB\")\n",
        "print(f\"SSIM:  {np.mean(all_ssim):.4f} Â± {np.std(all_ssim):.4f}\")\n",
        "print(f\"LPIPS: {np.mean(all_lpips):.4f} Â± {np.std(all_lpips):.4f}\")"
      ],
      "metadata": {
        "id": "analysis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}